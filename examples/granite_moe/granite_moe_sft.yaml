### model 30b /proj/checkpoints/mayank/30b-trial-p2-correct-lr/unsharded_model
model_name_or_path: /proj/checkpoints/mayank/120b-trial-p2/unsharded_model
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full

### dataset
dataset: alpaca_en
template: llama3
cutoff_len: 4096
overwrite_cache: true
preprocessing_num_workers: 64

### output 30b /proj/checkpoints/bathen/models/sft/30b-llamafactory-sft-5e-06-2ep
output_dir: /proj/checkpoints/bathen/models/sft/120b-llamafactory-sft-2e-06-2ep
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

### train - 30b - learning_rate: 5.0e-6, gradient_accumulation_steps: 2
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 2.0e-6
num_train_epochs: 2.0
lr_scheduler_type: linear
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
